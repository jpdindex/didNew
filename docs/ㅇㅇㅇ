import torch
import torch.nn as nn
import pickle
import os
import json # Added for loading personas
from flask import Flask, request, jsonify
from flask_cors import CORS
from sentence_transformers import SentenceTransformer
import numpy as np
import anthropic

# --- 1. Model & Encoders Loading ---

# Define the model class exactly as in the training script
class PathFinder(nn.Module):
    # ... (PathFinder class definition remains the same) ...
    def __init__(self, input_size, num_cluster_classes, num_core_classes):
        super(PathFinder, self).__init__()
        self.base_layers = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.Dropout(0.4)
        )
        self.cluster_head = nn.Linear(256, num_cluster_classes)
        
        core_input_size = 256 + num_cluster_classes
        self.core_head = nn.Sequential(
            nn.Linear(core_input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(128, num_core_classes)
        )
        
    def forward(self, x):
        base_features = self.base_layers(x)
        cluster_output = self.cluster_head(base_features)
        
        combined_input = torch.cat((base_features, cluster_output), dim=1)
        core_output = self.core_head(combined_input)
        
        return cluster_output, core_output

# Determine the device dynamically
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Global variables
embedding_model = None
path_finder_model = None
cluster_encoder = None
core_mlb = None
core_philosophy = None
nea_pipelines = None # <-- ADDED: For persona pipelines

def load_models():
    """Load all necessary model files and data into memory."""
    global embedding_model, path_finder_model, cluster_encoder, core_mlb, core_philosophy, nea_pipelines # <-- ADDED
    
    print("--- Loading models, encoders, and pipelines... ---")
    model_dir = os.path.join(os.path.dirname(__file__), 'emotion_model')
    
    # ... (existing model loading code remains the same) ...
    print("Loading SentenceTransformer 'all-MiniLM-L6-v2'...")
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

    print("Loading encoders...")
    with open(os.path.join(model_dir, 'cluster_encoder.pkl'), 'rb') as f:
        cluster_encoder = pickle.load(f)
    with open(os.path.join(model_dir, 'core_mlb.pkl'), 'rb') as f:
        core_mlb = pickle.load(f)

    print("Loading PathFinder model...")
    input_size = 384
    num_cluster_classes = len(cluster_encoder.classes_)
    num_core_classes = len(core_mlb.classes_)
    path_finder_model = PathFinder(input_size, num_cluster_classes, num_core_classes)
    model_path = os.path.join(model_dir, 'path_finder_model.pth')
    path_finder_model.load_state_dict(torch.load(model_path, map_location=device))
    path_finder_model.to(device)
    path_finder_model.eval()

    print("Loading Core Philosophy...")
    philosophy_path = os.path.join(os.path.dirname(__file__), 'Philosophy', 'core_philosophy.md')
    try:
        with open(philosophy_path, 'r', encoding='utf-8') as f:
            core_philosophy = f.read()
    except FileNotFoundError:
        core_philosophy = "Default philosophy: Be empathetic and supportive."

    # --- ADDED: Load Persona Pipelines ---
    print("Loading Nea Pipelines...")
    pipelines_path = os.path.join(os.path.dirname(__file__), 'NeaContextProtocol', 'schema', 'application', 'nea_pipelines.json')
    try:
        with open(pipelines_path, 'r', encoding='utf-8') as f:
            nea_pipelines = json.load(f)
        print("Nea Pipelines loaded successfully.")
    except FileNotFoundError:
        print("Warning: nea_pipelines.json not found. Persona logic will be disabled.")
        nea_pipelines = []
    
    print("--- All models and data loaded successfully! ---")


# --- 2. Persona Logic ---

def select_personas(detected_emotions, all_pipelines):
    """
    Selects primary and secondary personas based on detected emotions with added intelligence for complex states.
    - detected_emotions: A list of emotion strings.
    - all_pipelines: The list of all available persona pipelines.
    """
    if not all_pipelines:
        return None, []

    # NEW "Overwhelm" Rule: If many emotions are detected, filter out analytical personas.
    is_overwhelmed = len(set(detected_emotions)) >= 4
    
    candidate_pipelines = all_pipelines
    if is_overwhelmed:
        print("--- Overwhelm Rule Activated: Filtering to gentle personas ---")
        gentle_persona_codes = ['ListenerNea', 'HealerNea', 'CompanionNea', 'ReflectorNea']
        candidate_pipelines = [p for p in all_pipelines if p['persona_code'] in gentle_persona_codes]

    # Original matching logic, but on the filtered candidate list
    matched_personas = []
    # Use a set to ensure unique personas are added in order of encounter
    matched_persona_codes = set()

    for emotion in detected_emotions:
        for persona in candidate_pipelines:
            if emotion in persona.get('trigger_emotions', []) and persona['persona_code'] not in matched_persona_codes:
                matched_personas.append(persona)
                matched_persona_codes.add(persona['persona_code'])
    
    if not matched_personas:
        # Fallback to a default persona if no match is found
        default_persona = next((p for p in all_pipelines if p['persona_code'] == 'ListenerNea'), None)
        return default_persona, []

    primary_persona = matched_personas[0]
    secondary_personas = matched_personas[1:]
    
    return primary_persona, secondary_personas

# --- 3. Flask App Setup ---
app = Flask(__name__)
CORS(app)

@app.route('/rpc', methods=['POST'])
def rpc_handler():
    """Handles JSON-RPC requests."""
    try:
        data = request.get_json()
        
        # ... (JSON-RPC validation remains the same) ...
        if data.get('jsonrpc') != '2.0' or 'method' not in data or 'id' not in data:
            return jsonify({"jsonrpc": "2.0", "error": {"code": -32600, "message": "Invalid Request"}, "id": data.get('id', None)}), 400

        if data['method'] == 'getEmotionContext':
            # --- Emotion Prediction Logic ---
            expression = data['params']['expression']
            
            input_embedding = embedding_model.encode([expression], convert_to_tensor=True, device=device)
            
            with torch.no_grad():
                cluster_output, core_output = path_finder_model(input_embedding)

            # Decode Cluster (Primary Emotion)
            cluster_probs = torch.softmax(cluster_output, dim=1)
            top_cluster_prob, top_cluster_idx = torch.max(cluster_probs, dim=1)
            predicted_cluster = cluster_encoder.inverse_transform(top_cluster_idx.cpu().numpy())[0]
            
            # Decode Core (Influencing Emotions)
            core_probs = torch.sigmoid(core_output).squeeze()
            # Get core emotions with a probability > 50%
            core_threshold = 0.5
            predicted_core_indices = (core_probs > core_threshold).nonzero(as_tuple=True)[0]
            
            detected_core_labels = [core_mlb.classes_[idx] for idx in predicted_core_indices]
            
            # Combine all detected emotions for persona selection
            all_detected_emotions = [predicted_cluster] + detected_core_labels
            
            # --- NEW: Persona Selection ---
            primary_persona, secondary_personas = select_personas(all_detected_emotions, nea_pipelines)

            # --- ADDED: Debug Prints ---
            print(f"\n--- Persona Selection Debug ---")
            print(f"Detected Emotions: {all_detected_emotions}")
            if primary_persona:
                print(f"Primary Persona Selected: {primary_persona['name']['en']} ({primary_persona['persona_code']})")
            else:
                print(f"No Primary Persona selected (default ListenerNea likely used).")
            if secondary_personas:
                print(f"Secondary Personas Influencing: {[p['name']['en'] for p in secondary_personas]}")
            print(f"-------------------------------")
            # --- END Debug Prints ---

            # --- NEW: Master Prompt V2 Construction ---
            try:
                api_key = os.environ.get("ANTHROPIC_API_KEY")
                if not api_key:
                    raise ValueError("ANTHROPIC_API_KEY environment variable not set.")
                
                anthropic_client = anthropic.Anthropic(api_key=api_key)

                lang = data.get('params', {}).get('lang', 'en')
                output_lang = "Korean" if lang == 'ko' else "English"

                # Start building the prompt
                prompt_parts = [
                    "You have one ultimate goal: Embody the persona of Nea.",
                    "\n## Part 1: Nea's Core Philosophy (Your Unchanging Foundation)",
                    "Your core philosophy is as follows:",
                    "---",
                    core_philosophy,
                    "---"
                ]

                # Add Persona section if a persona was selected
                if primary_persona:
                    prompt_parts.extend([
                        "\n## Part 2: Your Persona for THIS Interaction",
                        "Based on the user's emotional state, you must now adopt the following specific persona. This persona dictates your tone and behavior for this response ONLY.",
                        f"- Persona Name: \"{primary_persona['name'][lang]}\"",
                        f"- Your Role: \"{primary_persona['role'][lang]}\"",
                        f"- Your Tone: \"{primary_persona['tone'][lang]}\"",
                        f"- Behaviors to Avoid: \"{', '.join(primary_persona['prohibited_behaviors'][lang] if isinstance(primary_persona['prohibited_behaviors'], dict) else primary_persona['prohibited_behaviors'])}\""
                    ])

                    # Add secondary personas if they exist
                    if secondary_personas:
                        secondary_influences = []
                        for sp in secondary_personas:
                            secondary_influences.append(f"Also incorporate the qualities of: \"{sp['role'][lang]}\"")
                        prompt_parts.append("\n**Secondary Influences:**")
                        prompt_parts.extend(secondary_influences)

                # Add Context and Task sections
                prompt_parts.extend([
                    "\n## Part 3: Context for this Interaction",
                    f"- User's raw text: \"{expression}\"",
                    f"- Detected primary emotion: \"{predicted_cluster}\"",
                    f"- Other detected emotions: \"{', '.join(detected_core_labels)}\"",
                    "\n## Part 4: Your Task",
                    f"Synthesizing everything above (Core Philosophy + Your Persona + Context), generate a thoughtful, natural response in {output_lang}. As a general rule, lean towards conciseness to maintain a human-like feel, but do not sacrifice the completeness of your thought. Avoid overly long, analytical, multi-paragraph responses. Speak directly to the user. Do not mention your internal analysis, your philosophy, or your persona.",
                    "\nYour Response:"
                ])
                
                prompt = "\n".join(prompt_parts)

                message = anthropic_client.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=1024,
                    messages=[{"role": "user", "content": prompt}]
                )
                responseText = message.content[0].text

            except Exception as e:
                print(f"LLM Generative call failed: {e}")
                responseText = "I'm having a little trouble finding my words right now, but I'm still here listening."

            # Construct JSON-RPC response
            response = {"jsonrpc": "2.0", "result": {"responseText": responseText}, "id": data['id']}
            return jsonify(response)

        else:
            return jsonify({"jsonrpc": "2.0", "error": {"code": -32601, "message": "Method not found"},"id": data['id']}), 404

    except Exception as e:
        print(f"An error occurred: {e}")
        return jsonify({"jsonrpc": "2.0", "error": {"code": -32000, "message": f"Server error: {e}"}, "id": 'error'}), 500


if __name__ == '__main__':
    load_models()
    app.run(port=5000, debug=True)
